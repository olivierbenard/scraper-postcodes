{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "super-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-worcester",
   "metadata": {},
   "source": [
    "## Récupération des landers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"http://germany.postcode.info/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(base)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opponent-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", {'class': 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "landers = []\n",
    "for div in divs:\n",
    "    temp = div.find_all(\"a\")\n",
    "    for a in temp:\n",
    "        landers.append(a['href'].replace('/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "viral-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baden-wurttemberg', 'bayern', 'berlin', 'brandenburg', 'bremen', 'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen', 'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen', 'sachsen-anhalt', 'schleswig-holstein', 'thuringen']\n"
     ]
    }
   ],
   "source": [
    "print(landers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-thompson",
   "metadata": {},
   "source": [
    "## Récupération de tous les codes postaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "realistic-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "behind-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    url_plz = base + f'/p/{i}'\n",
    "    res = requests.get(url_plz)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    divs = soup.find_all(\"div\", {'class': 'cnt'})\n",
    "    for div in divs:\n",
    "        temp = div.find_all(\"a\")\n",
    "        for a in temp:\n",
    "            postcode = a['href'].replace('/', '')\n",
    "            if len(postcode) == 5:\n",
    "                postcodes.append(a['href'].replace('/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "robust-answer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8312"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "impossible-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('postcodes.list', 'w') as file:\n",
    "    file.write('\\n'.join(postcodes))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-luxury",
   "metadata": {},
   "source": [
    "## Scraper pour une page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unusual-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get('http://germany.postcode.info/p/55278')\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adverse-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(souo):\n",
    "    content = soup.find_all(\"div\", {'class': 'cnt'})[0]\n",
    "    in_block = False\n",
    "    i_line = 0\n",
    "    lines = []\n",
    "    line = []\n",
    "    while True:\n",
    "        try:\n",
    "            content = content.next_element\n",
    "            if isinstance(content, bs4.element.Tag):\n",
    "                if content.name == 'strong' and not in_block:\n",
    "                    in_block = True\n",
    "                if content.name == 'br':\n",
    "                    if line != []:\n",
    "                        lines.append(line)\n",
    "                    line = []\n",
    "                    in_block = False\n",
    "                    i_line = 0\n",
    "            if isinstance(content, bs4.element.NavigableString) and in_block:\n",
    "                i_line += 1\n",
    "                if i_line == 1: # Ort\n",
    "                    line.append(content.strip())\n",
    "                if i_line == 2: # Plz (5-digit postcode)\n",
    "                    line.append(content.strip().replace(',', '').zfill(5))\n",
    "                if i_line == 3: # Lander\n",
    "                    line.append(content.strip())\n",
    "                if i_line == 4: # Kreise + GPS Lat, Long\n",
    "                    matched = re.findall(r'^,([^,]*)[^\\d]*(\\d{1,}.?\\d{0,})[^\\d]*(\\d{1,}.?\\d{0,})', content)\n",
    "                    temp = [line.append(item.strip()) for item in matched[0]]          \n",
    "        except:\n",
    "            break\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mineral-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = parser(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continental-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dalheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8267', '8.2953'], ['Dexheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8472', '8.3167'], ['Dolgesheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.7944', '8.2569'], ['Eimsheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.7958', '8.2894'], ['Friesenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8364', '8.2569'], ['Hahnheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8608', '8.2369'], ['Köngernheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8458', '8.2467'], ['Ludwigshöhe', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8167', '8.3436'], ['Mommenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8803', '8.265'], ['Selzen', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8606', '8.2553'], ['Uelversheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.81', '8.2886'], ['Undenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8381', '8.2189'], ['Weinolsheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8158', '8.2728']]\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-ability",
   "metadata": {},
   "source": [
    "### Factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lovely-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_postcode(postcode):\n",
    "    url = f'http://germany.postcode.info/p/{postcode}'\n",
    "    res = requests.get(url)\n",
    "    status_code = res.status_code \n",
    "    if status_code == 200:\n",
    "        page = BeautifulSoup(res.text, 'html.parser')\n",
    "        return parser(page)\n",
    "    print(f'{url} cannot be accessed: error {status_code}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "activated-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scrap_postcode('55278')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "atmospheric-thought",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dalheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8267', '8.2953'], ['Dexheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8472', '8.3167'], ['Dolgesheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.7944', '8.2569'], ['Eimsheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.7958', '8.2894'], ['Friesenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8364', '8.2569'], ['Hahnheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8608', '8.2369'], ['Köngernheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8458', '8.2467'], ['Ludwigshöhe', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8167', '8.3436'], ['Mommenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8803', '8.265'], ['Selzen', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8606', '8.2553'], ['Uelversheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.81', '8.2886'], ['Undenheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8381', '8.2189'], ['Weinolsheim', '55278', 'Rheinland-Pfalz', 'Mainz-Bingen', '49.8158', '8.2728']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pointed-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data == lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-willow",
   "metadata": {},
   "source": [
    "## Généralisation (scrap toutes les pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-sauce",
   "metadata": {},
   "source": [
    "### Récupération de tous les postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "associate-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('postcodes.list', 'r')\n",
    "postcodes = [line.replace('\\n', '') for line in file.readlines()]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "complicated-optimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8312"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-halloween",
   "metadata": {},
   "source": [
    "### Crawl toutes les pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-executive",
   "metadata": {},
   "source": [
    "3 à 4s pour scraper une page, 8312 pages à crawler, temps estimé à 9h... Le programme s'arrête prématurément avant le fin dans tous les cas, pour cause : la non gestion des timeouts lors des requests. Inutile d'aller plus loin avec cette solution naïve inefficace.\n",
    "\n",
    "**Solution :** Passer par `scrapy`.\n",
    "\n",
    "* https://pypi.org/project/Scrapy/\n",
    "* https://docs.scrapy.org/en/latest/index.html\n",
    "* https://www.youtube.com/watch?v=OJ8isyws2yw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "generic-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postcode 01067 beeing proceed (1/4)\n",
      "postcode 01069 beeing proceed (2/4)\n",
      "postcode 01097 beeing proceed (3/4)\n",
      "postcode 01099 beeing proceed (4/4)\n"
     ]
    }
   ],
   "source": [
    "postcodes = postcodes[:4] # on restreint volontairement le nombre de pages à crawler pour la démonstration\n",
    "with open('output.plz', 'a') as file:\n",
    "    n = len(postcodes)\n",
    "    for i, postcode in enumerate(postcodes):\n",
    "        print(f'postcode {postcode} beeing proceed ({i+1}/{n})')\n",
    "        data = scrap_postcode(postcode)\n",
    "        file.write('\\n'.join(','.join(item) for item in data) + \"\\n\")\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
